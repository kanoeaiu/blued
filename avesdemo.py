# -*- coding: utf-8 -*-
"""AvesDemo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FsCN77LpkU0sKPHPnxG-M1wnSs2AFWeP
"""

!git clone -q https://github.com/earthspecies/aves.git
!pip install -q wandb

# mount google
from google.colab import drive
drive.mount('/content/gdrive')

# 73df4adc60c07fd243c9aae5af5d6dc24878b803
!wandb login

training_file = '/content/gdrive/MyDrive/thesis/train.zip'
# !cp '/content/gdrive/My Drive/thesis/data/16smalltrain.zip' '/content'
!cp '/content/gdrive/MyDrive/thesis (1)/train.zip' '/content'
# !cp '/content/gdrive/My Drive/thesis/data/16smalldev.zip' '/content'
!unzip -q '/content/train.zip' -d '/content/train'
# !unzip '/content/16smalldev.zip' -d '/content/dev'

!pip install datasets>=1.18.3
!pip install transformers==4.24.0
!git clone https://github.com/facebookresearch/fairseq.git
!cd fairseq
!git checkout eda70379
!cd fairseq
# !cd fairseq && echo "import fairseq.checkpoint_utils" >> fairseq/__init__.py && pip install --editable ./
!cd fairseq && echo "import fairseq.checkpoint_utils # noqa" >> fairseq/__init__.py && pip install ./

!wget https://storage.googleapis.com/esp-public-files/aves/aves-base-bio.pt

!git clone https://github.com/mbari-org/oceansoundscape
# !pip install oceansoundscape

import os
os.environ['PYTHONPATH'] += ":/content/fairseq"

import numpy as np
import matplotlib.pyplot as plt
from datasets import load_dataset,Dataset,Audio
import fairseq
import torch
import torch.nn as nn
import fairseq.checkpoint_utils
import torch.optim as optim
from time import perf_counter
import wandb
import random

import torch.utils.checkpoint as checkpoint

class AvesClassifier(nn.Module):
    def __init__(self, model_path, num_classes, embeddings_dim=768):

        super().__init__()

        models, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([model_path])
        self.model = models[0]
        self.model.feature_extractor.requires_grad_(False)
        # use binary
        self.head = nn.Linear(in_features=embeddings_dim, out_features=num_classes)

        # nn.Sequential(
        #     nn.Linear(in_features=embeddings_dim, out_features=num_classes),
        #     nn.Softmax()
        # )

        self.loss_func = nn.CrossEntropyLoss()

    def forward(self, x, y=None):
        out = self.model.extract_features(x)[0]
        out = out.mean(dim=1)             # mean pooling, could remove
        logits = self.head(out)

        loss = None
        if y is not None:
            loss = self.loss_func(logits, y)

        return loss, logits


# Initialize an AVES classifier with 10 target classes
model = AvesClassifier(
    model_path='./aves-base-bio.pt',
    num_classes=2)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# if exporting need to put back on cpu
# command to watch GPU use (nvidia SMI)
# !watch -n 2 nvidia-smi
# look into opening colab terminal

print(model)

# count = 0
# for child in model.children():
#   if count==0:
#     # for child in child.children():
#     #   if count==0:
#     #     # print(child)
#     #     continue
#     #   count += 1
#     # print(count)
#     for param in child.parameters():
#       if count<4:
#         continue
#       count += 1
#       param.requires_grad = True

#     else:
#       for param in child.parameters():
#         param.requires_grad = True
#   count += 1

# torch.cuda.is_available()
def to_device(data, device):
    """Move tensor(s) to chosen device"""
    if isinstance(data, (list,tuple)):
        return [to_device(x, device) for x in data]
    return data.to(device, non_blocking=True)

# train_dataset = load_dataset("audiofolder", data_dir='/content/train/content/train', split='train[:10%]+train[-10%:]') # use 10%
# dev_dataset = load_dataset("audiofolder", data_dir='/content/train/content/train', split='train[10%:20%]+train[-20%:-10%]')

# train_dataloader= torch.utils.data.DataLoader(dataset=train_dataset, batch_size=16,shuffle=True, num_workers=2)
# test_dataloader= torch.utils.data.DataLoader(dataset=dev_dataset, batch_size=16,shuffle=True, num_workers=2)

def loadData(batch, samples):
  train_dataset = load_dataset("audiofolder", data_dir='/content/train/content', split=f'train[:{samples}]+train[-{samples}:]') # use 10%
  dev_dataset = load_dataset("audiofolder", data_dir='/content/train/content', split=f'train[{samples}:{2*samples}]+train[-{2*samples}:-{samples}]')

  train_dataloader= torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch,shuffle=True, num_workers=2)
  test_dataloader= torch.utils.data.DataLoader(dataset=dev_dataset, batch_size=batch,shuffle=True, num_workers=2)

  return train_dataloader, test_dataloader

def save_model(model):
  torch.save(model.state_dict(), 'blued.pt')
  !cp '/content/blued.pt' '/content/gdrive/MyDrive/thesis/data'

def train(learning_rate, num_epochs, train_dataloader, test_dataloader, model, num_accumulation_steps):
  criterion= nn.CrossEntropyLoss()
  optimizer=optim.Adam(model.parameters(), lr=learning_rate)

  t1_start = perf_counter()
  losses = np.zeros(num_epochs)

  for epoch in range(num_epochs):
      running_loss=0.0
      for i, data in enumerate(train_dataloader,0):
          audioarrs = data['audio']['array'].to(device).to(torch.float32)
          labels=data['label'].to(device)

          # zero the parameter gradients
          # optimizer.zero_grad()

          # forward
          loss,logits= model(audioarrs)
          #calculate the loss between the target and the actuals
          loss= criterion(logits, labels)
          #calculate loss
          loss = loss / num_accumulation_steps
          loss.backward()

          if ((i + 1) % num_accumulation_steps == 0) or (i + 1 == len(train_dataloader)):
            optimizer.step()
            # Reset gradient tensors
            optimizer.zero_grad()

      print('epoch %d - Loss %.3f' % (epoch +1, loss.item()))

      losses[epoch] = loss.item()
      accuracy = test_model(model, test_dataloader)
      wandb.log({"loss": loss.item(), "test_loss": accuracy})

  t1_end = perf_counter()
  print("Time for training using PyTorch %f" %(t1_end-t1_start))
  return losses

def test_model(model, test_dataloader):
  #evaluate on test dataset
  correct=0
  total=0
  t1_start=perf_counter()
  with torch.no_grad():
    count = 0
    for data in test_dataloader:
        count += 1
        if count%10 == 0:
          break
          print('.',end='',flush=True)
        audioarrs = data['audio']['array'].to(device).to(torch.float32)
        labels=data['label'].to(device)
        loss,logits= model(audioarrs)
        _,pred= torch.max(logits,1)
        total+=labels.size(0)
        correct += (pred == labels).sum().item()

  t1_end=perf_counter()
  accuracy = 100 * correct / total
  print("Eval accuracy using PyTorch is %.2f and execution time %.2f seconds" %((100 * (correct / total)), (t1_end-t1_start)))
  return accuracy

# start a new wandb run to track this script
def initdb(lr, epochs, numsamples, batchsize):
  wandb.init(
      # set the wandb project where this run will be logged
      project="blued",

      # track hyperparameters and run metadata
      config={
      "learning_rate": lr,
      "architecture": "transformer",
      "dataset": "smallbluedlowpass",
      "epochs": epochs,
      "num_samples": numsamples,
      "batch_size": batchsize,
      }
  )

# wandb.log({"acc": acc, "loss": loss})
# wandb.finish()
# make sure it's logging correctly (make sure data/loss is plottable)
# spectrogramify the new audio clips

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:1024"

def sweep():
  lr = 0.01
  epochs = 20
  batch = 2
  samples = 500
  grad_steps = 8
  train_dataloader, test_dataloader = loadData(samples, batch)
  initdb(lr, epochs, samples, batch)
  train(lr, epochs, train_dataloader, test_dataloader, model, grad_steps)
  test_model(model, test_dataloader)
  # save_model(model)
  wandb.finish()

sweep()

print(torch.cuda.memory_summary())

torch.cuda.empty_cache()
# model.to('cuda')

# sample = torch.from_numpy(np.array([train_dataset['train'][x]['audio']['array'] for x in range(2)]))
# sample = torch.from_numpy(train_dataset['train'][0]['audio']['array']).unsqueeze(0)
# print(sample)
# y = torch.tensor([0])
# loss,logits = model(sample,y)

# torch.rand((16_000)).unsqueeze(0)

# !cp '/content/gdrive/MyDrive/thesis/data/blued.pt' '/content/'
# model.load_state_dict(torch.load('/content/blued.pt'))
# model.eval()

"""### notebook updates

make everything into functions
organize data better
disable gradient calculation in evaluation?
other optimizations for cuda stuff
generate spectrograms of new data
build the validation set + full training set
build smaller training/val set to avoid unzipping full file
"""

#evaluate on test dataset
correct=0
total=0
t1_start=perf_counter()
with torch.no_grad():

    '''Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward().
    It will reduce memory consumption for computations that would otherwise have requires_grad=True.'''
count = 0
for  data in test_dataloader:
    count += 1
    if count%10 == 0:
      break
      print('.',end='',flush=True)
    audioarrs = data['audio']['array'] # list(map(lambda x: x['array'], data['audio']))
    labels=data['label']
    loss,logits= model(audioarrs)
    _,pred= torch.max(logits,1)
    total+=labels.size(0)
    correct += (pred == labels).sum().item()

t1_end=perf_counter()
print("Eval accuracy using PyTorch is %.2f and execution time %.2f seconds" %((100 * (correct / total)), (t1_end-t1_start)))

# function ClickConnect(){
# console.log("Working");
# document.querySelector("#top-toolbar > colab-connect-button").shadowRoot.querySelector("#connect").click();
# }
# setInterval(ClickConnect,60000)

# print(audio_dataset['train']['audio'][0])
# print(audio_dataset['train']['label'][0])